#!/bin/bash
accelerate launch --num_processes 2 finetune.py ./data/qa_dataset/ald.json \
    --ds_type=wizard30b \
    --lora_out_dir=./trained_loras/ald_batchsize128_qlora-8-16_cutoff384_epochs2_wizard30b \
    --llama_q4_config_dir=/workspace/text-generation-webui/models/TheBloke_WizardLM-30B-Uncensored-GPTQ \
    --llama_q4_model=/workspace/text-generation-webui/models/TheBloke_WizardLM-30B-GPTQ/WizardLM-30B-Uncensored-GPTQ-4bit--1g.act.order.safetensors \
    --mbatch_size=1 \
    --batch_size=128 \
    --epochs=2 \
    --lr=3e-4 \
    --cutoff_len=384 \
    --lora_r=8 \
    --lora_alpha=16 \
    --lora_dropout=0.05 \
    --warmup_steps=5 \
    --save_steps=4 \
    --save_total_limit=10 \
    --logging_steps=2 \
    --groupsize=-1 \
    --xformers \
    --backend=cuda\
    --grad_chckpt